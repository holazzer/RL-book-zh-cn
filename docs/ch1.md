# 第一章 简介

当我们考虑学习的本质时，通过与环境互动来学习的想法可能是我们首先想到的。 当婴儿玩耍，挥动手臂或环顾四周时，它没有明确的老师，但确实与周围环境有直接的感觉运动联系。行使这种联系会产生大量有关因果关系，行动后果以及为实现目标而应采取的行动的信息。在我们的一生中，此类互动无疑是有关我们的环境和我们自己的主要知识来源。 无论我们是学习驾驶汽车还是进行对话，我们都敏锐地意识到环境对我们所做工作的反应，并且我们试图通过行为来影响发生的事情。从互动中学习是几乎所有学习和智力理论的基础思想。

在本书中，我们探索了一种从交互中学习的计算方法。 我们不是直接对人或动物的学习方式进行理论化，而是主要探索理想化的学习环境并评估各种学习方法的有效性。我们采用人工智能研究人员或工程师的观点。我们探索有效解决科学或经济利益学习问题，通过数学分析或计算实验评估设计的机器设计。我们探索的方法称为强化学习，比其他机器学习方法更着重于通过交互进行的目标导向学习。

## 1.1 强化学习

强化学习是在学习如何做（如何将情况映射到行动），以便最大化数字奖励信号。不会告诉学习者要采取哪些行动，而必须通过尝试去发现哪些行动会产生最大的回报。在最有趣和最具挑战性的情况下，行动不仅会影响即时奖励，而且会影响下一个情况，并因此影响所有后续奖励。试错法和延迟奖励这两个特征是强化学习的两个最重要的区别特征。

强化学习是一个问题，是一类解决问题的好方法，还指研究这些问题的整个领域。在这三件事上使用单个名称很方便，但同时必须在概念上保持三者分开。在强化学习中，问题与解决方法之间的区别尤其重要。未能做出这种区分是造成许多困惑的根源。我们使用动态系统理论的思想来形式化描述强化学习的问题，特别是作为不完全已知的马尔可夫决策过程的最优控制。

形式化的细节必须等到第3章，但是其基本思想很简单。学习代理与环境进行即时互动，最终要达成一个目标，我们需要抓住学习体面对的最重要的方面。（原文：The details of this formalization must wait until Chapter 3, but the basic idea is simply to capture the most important aspects of the real problem facing a learning agent interacting over time with its environment to achieve a goal. 自行理解吧，我也只能翻到这了。）学习代理必须能够在某种程度上感知其环境状态，并且必须能够采取影响状态的行动。代理还必须具有与环境状态有关的一个或多个目标。马尔可夫决策过程旨在以最简单的形式包括这三个方面（感觉，行动和目标）。任何非常适合解决此类问题的方法，我们都认为是强化学习方法。


强化学习与监督学习不同，后者是机器学习领域的最新研究。监督学习是从知识渊博的外部监督者提供的一系列带标签的示例中学习。每个示例都是对情况的描述以及系统应针对该情况采取的正确操作的规范（标签），该规范通常用于标识该情况所属的类别。这种学习的目的是使系统推断或概括其响应，以使其在训练集中不存在的情况下也能正确运行。这是一种重要的学习方式，但仅靠它还是不够的。在互动问题中，想要获得既正确，又能够代表所有可能遇到的情况的样本，是不切实际的。在未知的领域（对未知领域的学习对人们更有用），代理必须能够从自己的经验中学到东西。

强化学习也与机器学习研究人员所谓的无监督学习不同，后者通常是寻找隐藏在未标记数据集合中的结构。监督学习和无监督学习这两个术语似乎已对机器学习范例进行了详尽的分类，但事实并非如此。尽管人们可能会倾向于将强化学习视为一种无监督的学习，因为它并不依赖于正确行为的例子，但是强化学习正在尝试最大化奖励信号，而不是试图寻找隐藏的结构。探究代理人经验中的结构肯定对强化学习很有用，但其本身并不能解决最大化奖励信号的强化学习问题。因此，我们将强化学习与监督学习和非监督学习以及其他范式一起，视为第三种机器学习范式。


在强化学习而不是其他类型的学习中出现的挑战之一是**探索与利用的平衡问题(the trade-of between exploration and exploitation)**。为了获得很多奖励，强化学习代理必须更倾向于过去尝试过的，被认为可以有效产生奖励的动作。但是要发现此类操作，它必须尝试以前未选择的操作。代理必须利用已经获得的经验来获得奖励，但也必须进行探索以便将来做出更好的选择。难题在于，在不失败的情况下，不能只进行探索或只进行利用。代理必须尝试各种行动，并逐步支持那些看起来最好的行动。对于随机任务，必须多次尝试每个操作才能获得对其预期回报的可靠估计。数十年来，数学家对探索与利用的困境进行了深入研究，但仍未解决。现在，我们只是简单地注意到，平衡探索与利用的整个问题甚至都没有出现在有监督和无监督的学习中，至少在这些范式的最纯粹形式中并没有出现。强化学习的另一个关键特征是，它明确考虑了目标导向的主体与不确定环境交互的整个问题。这与许多考虑子问题却未解决它们可能如何适应更大情况的方法形成对比。例如，我们提到很多机器学习研究都与监督学习有关，而没有明确说明这种能力最终将如何有用。其他研究人员已经开发了具有总体目标的计划理论，但没有考虑计划在实时决策中的作用，也没有考虑计划必要的预测模型从何而来的问题。尽管这些方法已产生许多有用的结果，但它们对孤立的子问题的关注是一个重大局限。


增强学习采取相反的方法，从完整的，交互式的，寻求目标的代理开始。所有强化学习代理都有明确的目标，可以感知其环境的各个方面，并可以选择影响其环境的行动。而且，通常从一开始就假定代理必须运行，尽管对其所面临的环境存在很大的不确定性。当强化学习涉及计划时，它必须解决计划与实时行动选择之间的相互作用，以及如何获取和改善环境模型的问题。当强化学习涉及监督学习时，它是出于特定原因而决定哪些能力是重要的，哪些不是。为了推动学习进程，必须分离并分开研究重要的子问题，这些子问题应该是完整的、互动性的、寻求目标的代理的一部分，尽管这个代理自己的完整目标还不能定下来。（原文：For learning research to make progress, important subproblems have to be isolated and studied, but they should be subproblems that play clear roles in complete, interactive, goal-seeking agents, even if all the details of the complete agent cannot yet be filled in.）


我们说一个完整的，交互式的，寻求目标的代理，我们并不总是指一个完整的有机体或机器人。这些显然是示例，但是完整的，交互式的，寻求目标的代理程序也可以是更大的行为系统的组成部分。在这种情况下，代理直接与大型系统的其余部分交互，而与大型系统的环境间接交互。举一个一个简单的例子：一个代理，它监视机器人电池的电量并将其发送到机器人的控制体系结构。该代理的环境是机器人的其余部分以及机器人的环境。人们必须超越代理及其环境的最明显示例，才能体会到强化学习框架的普遍性。


现代强化学习最令人兴奋的方面之一是它与其他工程学和科学学科的实质性和富有成果的互动。强化学习是人工智能和机器学习数十年来趋势的一部分，该趋势是与统计，优化和其他数学学科更好地集成。例如，一些强化学习方法使用参数化近似器进行学习的能力解决了运筹学和控制理论中的经典“维数诅咒”。更明显的是，强化学习也与心理学和神经科学产生了强烈的相互作用，双向都有着巨大的好处。在所有形式的机器学习中，强化学习最接近人类和其他动物所做的那种学习，强化学习的许多核心算法最初都是受生物学习系统启发的。强化学习还通过动物学习的心理学模型（与某些经验数据更好地匹配）以及大脑奖励系统各部分的有影响力的模型而有所回报。本书的主体提出了与工程和人工智能有关的强化学习的思想，并在第14章和第15章中概述了与心理学和神经科学的联系。


最后，强化学习也是人工智能向简单通用发展的大趋势的一部分。自1960年代后期以来，许多人工智能研究人员以为没有通用的原理可以发现，而智能只是大量特殊的技巧，程序和启发式方法。有时有人说，如果我们只要将足够的相关事实放入一台机器中，例如一百万或十亿，它就会变得智能。基于一般原理的方法（例如搜索或学习）被称为“弱方法”，而基于特定知识的方法被称为“强方法”。这种观点在今天仍然很普遍，但并不占主导地位。从我们的角度来看，这还为时过早：在寻求一般原则时得出的结论太少，以至于没有结论。现代人工智能现在包括许多研究，以寻找学习，搜索和决策的一般原理。目前尚不清楚摆将向后摆动多远，但强化学习研究无疑是向更简单和更少的人工智能通用原理回摆的一部分。



## 1.2 例子
理解强化学习的一种好方法是考虑一些指导其发展的示例和可能的应用。

+ 国际象棋大师的走棋。通过计划（预期可能的反应和对这些反应的回击）以及对特定位置和动作的合意性的立即，直观的判断，做出选择。

+ 自适应控制器实时调整炼油厂的运行参数。控制器根据指定的边际成本，优化收益/成本/质量之间的折衷策略，而不必严格遵守工程师最初建议的设定参数。

+ 羚羊出生后几分钟就挣扎着站起来。半小时后，它可以以每小时20英里的速度运行。

+ 移动机器人决定是否应该进入一个新房间以寻找更多的垃圾来收集，还是开始尝试返回电池充电站。它根据电池的当前电量以及过去能够快速便捷地找到充电器的经验来做出决定。

+ 菲尔准备早餐。即使是这种表面上平凡的活动，也显示出复杂的条件行为网络和目标-次目标之间的相互联系：走到橱柜，打开它，选择一个谷物盒，然后伸手去拿，抓紧并取回盒子。经过其他复杂的，经过调整的交互行为序列，才能获得碗，勺子和牛奶纸箱。每个步骤都涉及一系列的眼球运动，以获取信息并指导伸手和运动。对于如何携带物品或在获取其他物品之前先将其中的一些物品运送到餐桌上，会不断做出快速的判断。每个步骤均以目标为指导，例如握住勺子或上冰箱，并达到其他目标，例如一旦准备好谷物就可以将勺子与食物一起食用，并最终获得营养。不管他是否意识到，菲尔都在访问有关其身体状况的信息，该信息决定了他的营养需求，饥饿程度和食物偏好。


这些例子展示的特征都很基本，所以很容易被忽视。他们都牵扯到一个做决策的代理和环境之间的*互动*，在环境中，代理期望达成自己的*目标*，但是环境中有*不确定性*。代理的行为可以影响环境的未来状态（比如，下一个棋子的位置，炼油厂的库存，机器人的下一个位置，电池的充电情况等。），从而影响代理接下来的行为和机会。正确的选择需要考虑行为间接的，延迟的后果，所以需要前瞻和计划。


同时，在所有这些示例中，动作的效果无法完全预测。因此，代理必须经常监视其环境并做出适当的反应。例如，菲尔必须注意倒入谷物碗的牛奶，以防止牛奶溢出。所有这些示例都涉及明确的目标，即代理可以根据其直接感知的结果来判断实现目标的进度。下象棋的人知道他是否获胜，炼油厂的控制者知道产生了多少石油，小羚羊知道它摔倒了，移动机器人知道它的电池何时耗尽，而菲尔知道他是否在享用早餐。


在所有这些示例中，代理可以利用其经验来随着时间的推移改善其性能。下象棋的人改善了他用来评估位置的直觉，从而改善了他的下法；小羚羊提高了它的跑步效率；菲尔学会精简做早餐的流程。代理带到这个任务的知识，可以是之前经历中学到的，也可以是进化得来的，都会影响什么是有用的或者什么容易学，但是和环境的交互对于为完成任务而调整自己的行为是至关重要的。

## 1.3 强化学习的元素

除了主体和环境之外，强化学习系统还有四个主要元素：策略，奖励信号，价值函数以及可选的环境模型。

**策略(policy)**规定了学习代理在给定时间的行为。粗略地说，策略是从感知到的环境状态到在这些状态下要采取的行动的映射。它对应于心理学上被称为一套刺激-反应规则或联想的东西。在某些情况下，该策略可能是简单的函数或查找表，而在其他情况下，它可能涉及大量的计算，例如搜索过程。该政策是强化学习代理的核心，就其本身而言，它足以确定行为。通常，策略可能是随机的，但是给定了每个行为的概率。


**奖励信号(reward signal)**定义了强化学习问题的目标。在每个时间点上，环境都会向强化学习代理发送一个代表奖励的数字。代理的唯一目标是在长期内获得最大的总回报。奖励信号因此定义了对代理而言什么是好事，什么是坏事。在生物系统中，我们可能认为奖励类似于愉悦或痛苦的经历。它们是代理面临的问题的直接和定义特征。奖励信号是更改策略的主要依据；如果该策略选择的操作之后是低奖励，则将来可能会更改该策略以选择该情况下的其他一些操作。一般而言，奖励信号可能是环境状态和所采取行动的随机函数。


奖励信号在近期意义上指示什么是好的，而**价值函数(value function)**则从长远来看指示什么是好的。粗略地说，状态的价值是代理从该状态开始可以期望在未来积累的总奖励金额。奖励决定了环境状态的即时，内在的可取性。而值表示考虑了可能遵循的状态以及这些状态中可获得的奖励后，对状态的**长期**期望。例如，一个状态可能总是产生低的立即回报，但仍然具有很高的价值，因为跟随着它总是有其他产生高回报的状态。情况也可能正好相反。打个比方，奖励有点像愉悦感（如果很高）和痛苦（如果很低），而价值则对应于我们对我们的环境处于特定状态的满意程度或满意度的更精致和有远见的判断。

感觉是主要的，而价值（作为对奖励的预测）则是次要的。没有奖励就不可能有价值，而估计价值的唯一目的就是获得更多的奖励。但是，在制定和评估决策时，我们最关心的是价值。根据价值判断做出行动选择。寻求最高价值而不是最高回报的状态，因为从长远来看，这些行动会为我们带来最大的回报。不幸的是，确定价值比确定奖励要困难得多。奖励基本上是由环境直接给予的，但是价值必须根据代理在其整个生命周期中进行的观察序列来不断地估计和重新估计。实际上，我们考虑的几乎所有强化学习算法中最重要的组成部分是一种科学估算价值的方法。价值评估的中心作用可以说是过去六十年来有关强化学习的最重要的知识。


一些强化学习系统包括第四个要素，**环境模型(a model of the environment)**。这是模仿环境行为的东西，或更笼统地说，是可以推断出环境的行为方式。例如，给定状态和动作，模型可以预测所得的下一个状态和下一个奖励。模型用于**计划(planning)**，我们指的是通过实际考虑可能出现的未来情况来决定行动方案的任何方式。使用模型和计划来解决强化学习问题的方法被称为**基于模型的(model-based)**方法，与简单的**无模型(model-free)**方法（明确地说，是试错学习(trial-and-error)）相反，它们被认为与计划相反。在第8章中，我们探索了强化学习系统，该系统可以通过试错法同时学习，学习环境模型并将该模型用于计划。现代强化学习涵盖了从低层的试错学习到高层的深思熟虑的计划的范围。


## 1.4 增强学习的局限和本书的讨论范围

强化学习在很大程度上依赖于状态的概念，即作为策略和价值函数的输入，以及作为模型输入和输出的状态。非正式地，我们可以将状态视为传递给代理的某种信号，以了解环境在特定时间的状况。状态的形式化定义是由第3章中提出的马尔可夫决策过程框架给出的。但是，更广泛地讲，我们鼓励读者遵循非正式的含义，并认为状态是代理可以获取的关于环境的任何信息。实际上，我们假设状态信号是由某种预处理系统产生的，该系统名义上是代理所在环境的一部分。在本书中，我们不讨论构造，更改或学习状态信号的问题（第17.3节中只是简要介绍）。我们采取这种方法不是因为我们认为状态表示不重要，而是为了充分关注决策问题。换句话说，本书中我们关注的不是设计状态信号，而是根据可用的状态信号来决定采取何种行动。


本书中考虑的大多数强化学习方法都是围绕估计值函数构造的，但是解决增强学习问题并非绝对必要这样做。例如，诸如遗传算法，遗传编程，模拟退火和其他优化方法之类的求解方法永远不会估计值函数。这些方法应用多个静态策略，每个策略在较长的时间段内与单独的环境实例进行交互。获得最大奖励的策略及其随机变化将被延续到下一代策略，然后重复该过程。我们之所以称其为进化方法，是因为它们的操作类似于生物进化产生具有熟练行为的生物的方式，即使它们在一生中没有学习。如果策略的空间足够小，或者可以进行构造使得好的策略很容易找到，或者有足够的时间进行搜索，那么进化方法就可以有效地发挥作用。此外，进化方法在学习主体无法感知其环境完整状态的问题上具有优势。


我们的重点是在与环境交互时学习的强化学习方法，而进化方法则无法做到。在许多情况下，能够利用个体行为交互的细节的方法可能比进化方法更为有效。进化方法忽略了强化学习问题的许多有用结构：它们没有使用他们正在寻找的政策是从状态到行动的函数这一事实；他们不会注意到一个个体在其一生中会经历哪个状态，或者选择了哪个动作。在某些情况下，此信息可能会引起误解（例如，接收到的状态是错误的），但更多情况下，它应该可以使搜索更有效。尽管进化和学习具有许多功能并且可以自然地协同工作，但我们并不认为进化方法本身特别适合强化学习问题，因此，我们在本书中没有介绍它们。


## 1.5 一个扩展的例子：井字棋



