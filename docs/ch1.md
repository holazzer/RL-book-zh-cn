# 第一章 简介

当我们考虑学习的本质时，通过与环境互动来学习的想法可能是我们首先想到的。 当婴儿玩耍，挥动手臂或环顾四周时，它没有明确的老师，但确实与周围环境有直接的感觉运动联系。行使这种联系会产生大量有关因果关系，行动后果以及为实现目标而应采取的行动的信息。在我们的一生中，此类互动无疑是有关我们的环境和我们自己的主要知识来源。 无论我们是学习驾驶汽车还是进行对话，我们都敏锐地意识到环境对我们所做工作的反应，并且我们试图通过行为来影响发生的事情。从互动中学习是几乎所有学习和智力理论的基础思想。

在本书中，我们探索了一种从交互中学习的计算方法。 我们不是直接对人或动物的学习方式进行理论化，而是主要探索理想化的学习环境并评估各种学习方法的有效性。我们采用人工智能研究人员或工程师的观点。我们探索有效解决科学或经济利益学习问题，通过数学分析或计算实验评估设计的机器设计。我们探索的方法称为强化学习，比其他机器学习方法更着重于通过交互进行的目标导向学习。

## 1.1 强化学习

强化学习是在学习如何做（如何将情况映射到行动），以便最大化数字奖励信号。不会告诉学习者要采取哪些行动，而必须通过尝试去发现哪些行动会产生最大的回报。在最有趣和最具挑战性的情况下，行动不仅会影响即时奖励，而且会影响下一个情况，并因此影响所有后续奖励。试错法和延迟奖励这两个特征是强化学习的两个最重要的区别特征。

强化学习是一个问题，是一类解决问题的好方法，还指研究这些问题的整个领域。在这三件事上使用单个名称很方便，但同时必须在概念上保持三者分开。在强化学习中，问题与解决方法之间的区别尤其重要。未能做出这种区分是造成许多困惑的根源。我们使用动态系统理论的思想来形式化描述强化学习的问题，特别是作为不完全已知的马尔可夫决策过程的最优控制。

形式化的细节必须等到第3章，但是其基本思想很简单。学习代理与环境进行即时互动，最终要达成一个目标，我们需要抓住学习体面对的最重要的方面。（原文：The details of this formalization must wait until Chapter 3, but the basic idea is simply to capture the most important aspects of the real problem facing a learning agent interacting over time with its environment to achieve a goal. 自行理解吧，我也只能翻到这了。）学习代理必须能够在某种程度上感知其环境状态，并且必须能够采取影响状态的行动。代理还必须具有与环境状态有关的一个或多个目标。马尔可夫决策过程旨在以最简单的形式包括这三个方面（感觉，行动和目标）。任何非常适合解决此类问题的方法，我们都认为是强化学习方法。


强化学习与监督学习不同，后者是机器学习领域的最新研究。监督学习是从知识渊博的外部监督者提供的一系列带标签的示例中学习。每个示例都是对情况的描述以及系统应针对该情况采取的正确操作的规范（标签），该规范通常用于标识该情况所属的类别。这种学习的目的是使系统推断或概括其响应，以使其在训练集中不存在的情况下也能正确运行。这是一种重要的学习方式，但仅靠它还是不够的。在互动问题中，想要获得既正确，又能够代表所有可能遇到的情况的样本，是不切实际的。在未知的领域（对未知领域的学习对人们更有用），代理必须能够从自己的经验中学到东西。

强化学习也与机器学习研究人员所谓的无监督学习不同，后者通常是寻找隐藏在未标记数据集合中的结构。监督学习和无监督学习这两个术语似乎已对机器学习范例进行了详尽的分类，但事实并非如此。尽管人们可能会倾向于将强化学习视为一种无监督的学习，因为它并不依赖于正确行为的例子，但是强化学习正在尝试最大化奖励信号，而不是试图寻找隐藏的结构。探究代理人经验中的结构肯定对强化学习很有用，但其本身并不能解决最大化奖励信号的强化学习问题。因此，我们将强化学习与监督学习和非监督学习以及其他范式一起，视为第三种机器学习范式。


在强化学习而不是其他类型的学习中出现的挑战之一是**探索与利用的平衡问题(the trade-of between exploration and exploitation)**。为了获得很多奖励，强化学习代理必须更倾向于过去尝试过的，被认为可以有效产生奖励的动作。但是要发现此类操作，它必须尝试以前未选择的操作。代理必须利用已经获得的经验来获得奖励，但也必须进行探索以便将来做出更好的选择。难题在于，在不失败的情况下，不能只进行探索或只进行利用。代理必须尝试各种行动，并逐步支持那些看起来最好的行动。对于随机任务，必须多次尝试每个操作才能获得对其预期回报的可靠估计。数十年来，数学家对探索与利用的困境进行了深入研究，但仍未解决。现在，我们只是简单地注意到，平衡探索与利用的整个问题甚至都没有出现在有监督和无监督的学习中，至少在这些范式的最纯粹形式中并没有出现。强化学习的另一个关键特征是，它明确考虑了目标导向的主体与不确定环境交互的整个问题。这与许多考虑子问题却未解决它们可能如何适应更大情况的方法形成对比。例如，我们提到很多机器学习研究都与监督学习有关，而没有明确说明这种能力最终将如何有用。其他研究人员已经开发了具有总体目标的计划理论，但没有考虑计划在实时决策中的作用，也没有考虑计划必要的预测模型从何而来的问题。尽管这些方法已产生许多有用的结果，但它们对孤立的子问题的关注是一个重大局限。


增强学习采取相反的方法，从完整的，交互式的，寻求目标的代理开始。所有强化学习代理都有明确的目标，可以感知其环境的各个方面，并可以选择影响其环境的行动。而且，通常从一开始就假定代理必须运行，尽管对其所面临的环境存在很大的不确定性。当强化学习涉及计划时，它必须解决计划与实时行动选择之间的相互作用，以及如何获取和改善环境模型的问题。当强化学习涉及监督学习时，它是出于特定原因而决定哪些能力是重要的，哪些不是。为了推动学习进程，必须分离并分开研究重要的子问题，这些子问题应该是完整的、互动性的、寻求目标的代理的一部分，尽管这个代理自己的完整目标还不能定下来。（原文：For learning research to make progress, important subproblems have to be isolated and studied, but they should be subproblems that play clear roles in complete, interactive, goal-seeking agents, even if all the details of the complete agent cannot yet be filled in.）


我们说一个完整的，交互式的，寻求目标的代理，我们并不总是指一个完整的有机体或机器人。这些显然是示例，但是完整的，交互式的，寻求目标的代理程序也可以是更大的行为系统的组成部分。在这种情况下，代理直接与大型系统的其余部分交互，而与大型系统的环境间接交互。举一个一个简单的例子：一个代理，它监视机器人电池的电量并将其发送到机器人的控制体系结构。该代理的环境是机器人的其余部分以及机器人的环境。人们必须超越代理及其环境的最明显示例，才能体会到强化学习框架的普遍性。


现代强化学习最令人兴奋的方面之一是它与其他工程学和科学学科的实质性和富有成果的互动。强化学习是人工智能和机器学习数十年来趋势的一部分，该趋势是与统计，优化和其他数学学科更好地集成。例如，一些强化学习方法使用参数化近似器进行学习的能力解决了运筹学和控制理论中的经典“维数诅咒”。更明显的是，强化学习也与心理学和神经科学产生了强烈的相互作用，双向都有着巨大的好处。在所有形式的机器学习中，强化学习最接近人类和其他动物所做的那种学习，强化学习的许多核心算法最初都是受生物学习系统启发的。强化学习还通过动物学习的心理学模型（与某些经验数据更好地匹配）以及大脑奖励系统各部分的有影响力的模型而有所回报。本书的主体提出了与工程和人工智能有关的强化学习的思想，并在第14章和第15章中概述了与心理学和神经科学的联系。


最后，强化学习也是人工智能向简单通用发展的大趋势的一部分。自1960年代后期以来，许多人工智能研究人员以为没有通用的原理可以发现，而智能只是大量特殊的技巧，程序和启发式方法。有时有人说，如果我们只要将足够的相关事实放入一台机器中，例如一百万或十亿，它就会变得智能。基于一般原理的方法（例如搜索或学习）被称为“弱方法”，而基于特定知识的方法被称为“强方法”。这种观点在今天仍然很普遍，但并不占主导地位。从我们的角度来看，这还为时过早：在寻求一般原则时得出的结论太少，以至于没有结论。现代人工智能现在包括许多研究，以寻找学习，搜索和决策的一般原理。目前尚不清楚摆将向后摆动多远，但强化学习研究无疑是向更简单和更少的人工智能通用原理回摆的一部分。



## 1.2 例子
理解强化学习的一种好方法是考虑一些指导其发展的示例和可能的应用。

+ 国际象棋大师的走棋。通过计划（预期可能的反应和对这些反应的回击）以及对特定位置和动作的合意性的立即，直观的判断，做出选择。

+ 自适应控制器实时调整炼油厂的运行参数。控制器根据指定的边际成本，优化收益/成本/质量之间的折衷策略，而不必严格遵守工程师最初建议的设定参数。

+ 羚羊出生后几分钟就挣扎着站起来。半小时后，它可以以每小时20英里的速度运行。

+ 移动机器人决定是否应该进入一个新房间以寻找更多的垃圾来收集，还是开始尝试返回电池充电站。它根据电池的当前电量以及过去能够快速便捷地找到充电器的经验来做出决定。

+ 菲尔准备早餐。即使是这种表面上平凡的活动，也显示出复杂的条件行为网络和目标-次目标之间的相互联系：走到橱柜，打开它，选择一个谷物盒，然后伸手去拿，抓紧并取回盒子。经过其他复杂的，经过调整的交互行为序列，才能获得碗，勺子和牛奶纸箱。每个步骤都涉及一系列的眼球运动，以获取信息并指导伸手和运动。对于如何携带物品或在获取其他物品之前先将其中的一些物品运送到餐桌上，会不断做出快速的判断。每个步骤均以目标为指导，例如握住勺子或上冰箱，并达到其他目标，例如一旦准备好谷物就可以将勺子与食物一起食用，并最终获得营养。不管他是否意识到，菲尔都在访问有关其身体状况的信息，该信息决定了他的营养需求，饥饿程度和食物偏好。


这些例子展示的特征都很基本，所以很容易被忽视。他们都牵扯到一个做决策的代理和环境之间的*互动*，在环境中，代理期望达成自己的*目标*，但是环境中有*不确定性*。代理的行为可以影响环境的未来状态（比如，下一个棋子的位置，炼油厂的库存，机器人的下一个位置，电池的充电情况等。），从而影响代理接下来的行为和机会。正确的选择需要考虑行为间接的，延迟的后果，所以需要前瞻和计划。


同时，在所有这些示例中，动作的效果无法完全预测。因此，代理必须经常监视其环境并做出适当的反应。例如，菲尔必须注意倒入谷物碗的牛奶，以防止牛奶溢出。所有这些示例都涉及明确的目标，即代理可以根据其直接感知的结果来判断实现目标的进度。下象棋的人知道他是否获胜，炼油厂的控制者知道产生了多少石油，小羚羊知道它摔倒了，移动机器人知道它的电池何时耗尽，而菲尔知道他是否在享用早餐。


在所有这些示例中，代理可以利用其经验来随着时间的推移改善其性能。下象棋的人改善了他用来评估位置的直觉，从而改善了他的下法；小羚羊提高了它的跑步效率；菲尔学会精简做早餐的流程。代理带到这个任务的知识，可以是之前经历中学到的，也可以是进化得来的，都会影响什么是有用的或者什么容易学，但是和环境的交互对于为完成任务而调整自己的行为是至关重要的。

## 1.3 强化学习的元素

